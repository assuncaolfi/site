[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Home",
    "section": "",
    "text": "Hello! I’m Luís Assunção1 and this is my website.\n1 I’m a statistician and data scientist – my goal is to help people make better decisions under uncertainty. To achieve this, I design experiments, infer causal relationships, model probabilities, and more.\nI also enjoy hiking, music and woodworking. I live with my partner and our ginger cat2 in Belo Horizonte, Brazil.\n2"
  },
  {
    "objectID": "index.html#blog",
    "href": "index.html#blog",
    "title": "Home",
    "section": "Blog ",
    "text": "Blog"
  },
  {
    "objectID": "cv.html",
    "href": "cv.html",
    "title": "Curriculum Vitae",
    "section": "",
    "text": "Email /  GitHub /  LinkedIn /  Website\n\n\n\n\nStaff Data Scientist | April 2020 - present\n\nDeveloped an in-house AB hierarchical testing framework with optional stopping\nConsulted for and developed randomized controlled trials\nEstimated causal effects in non-randomized experiments\nEstimated pricing elasticity for digital products using multilevel models\nClassified evergreen vs launching sales strategies using hidden state models\nImproved quality of course assigments using Item Response Theory models\n\n\n\n\nData Scientist | Oct 2018 - March 2020\n\nConsulted for companies such as AB InBev and GTB in statistical projects\nModeled spatial pricing elasticity for beverages using Gaussian Processes\nEstimated revenue attribution in multi-touchpoint marketing campaigns\n\n\n\n\nIntern | 2015 - 2017\n\nCollected, wrangled and described survey data\nResearched policies to advance human rights in the digital matters\n\n\n\n\n\n\n\nFederal University of Minas Gerais (UFMG) | Belo Horizonte, MG - Brazil | 2017 - 2021\n\nResearched and authored a reproducible monograph (in portuguese with an abstract in english) on exponential random graphs applied to epidemiology\nCo-authored Frequency and burden of neurological manifestations upon hospital presentation in COVID-19 patients: Findings from a large Brazilian cohort\n\n\n\n\n\n\n\nPosts on data analysis using tools such as Python, polars, pymc, pulp, seaborn:\n\nPicking a fantasy football team: In this post, I delve into the data for the 2022 season of a brazilian fantasy football league; formulate a mixed integer linear program to pick the optimal team; and present initial concepts for forecasting player scores using mixed effects linear models.\nDecomposable non-monotonic models: In this post, I compare empirical and parametric approaches to model non- monotonic relationships using a Digit Span verbal working memory cognitive test dataset.\n\n\n\n\n\nsite: My website and blog post codes using Quarto Markdown\nmldc2020: Recommendation system and 7th place solution to the Mercado Libre Data Challenge 2020\nrstanbtm: Biterm Topic Model implementation in Stan\nqlm: Generate predictive SQL queries from linear models in R\ntophat: Scheduled shell script to fetch and save fantasy football data\n\n\n\n\n\nPod e Dev podcast episode, where I talk (in portuguese) about the challenges in pricing digital products and causal assumptions we made to overcome these challenges in our model at Hotmart. We also discuss good and bad use cases for large language models, as well as how models with 2 parameters can be as useful as models with 200 million parameters."
  },
  {
    "objectID": "cv.html#employment",
    "href": "cv.html#employment",
    "title": "Curriculum Vitae",
    "section": "",
    "text": "Staff Data Scientist | April 2020 - present\n\nDeveloped an in-house AB hierarchical testing framework with optional stopping\nConsulted for and developed randomized controlled trials\nEstimated causal effects in non-randomized experiments\nEstimated pricing elasticity for digital products using multilevel models\nClassified evergreen vs launching sales strategies using hidden state models\nImproved quality of course assigments using Item Response Theory models\n\n\n\n\nData Scientist | Oct 2018 - March 2020\n\nConsulted for companies such as AB InBev and GTB in statistical projects\nModeled spatial pricing elasticity for beverages using Gaussian Processes\nEstimated revenue attribution in multi-touchpoint marketing campaigns\n\n\n\n\nIntern | 2015 - 2017\n\nCollected, wrangled and described survey data\nResearched policies to advance human rights in the digital matters"
  },
  {
    "objectID": "cv.html#education",
    "href": "cv.html#education",
    "title": "Curriculum Vitae",
    "section": "",
    "text": "Federal University of Minas Gerais (UFMG) | Belo Horizonte, MG - Brazil | 2017 - 2021\n\nResearched and authored a reproducible monograph (in portuguese with an abstract in english) on exponential random graphs applied to epidemiology\nCo-authored Frequency and burden of neurological manifestations upon hospital presentation in COVID-19 patients: Findings from a large Brazilian cohort"
  },
  {
    "objectID": "cv.html#examples",
    "href": "cv.html#examples",
    "title": "Curriculum Vitae",
    "section": "",
    "text": "Posts on data analysis using tools such as Python, polars, pymc, pulp, seaborn:\n\nPicking a fantasy football team: In this post, I delve into the data for the 2022 season of a brazilian fantasy football league; formulate a mixed integer linear program to pick the optimal team; and present initial concepts for forecasting player scores using mixed effects linear models.\nDecomposable non-monotonic models: In this post, I compare empirical and parametric approaches to model non- monotonic relationships using a Digit Span verbal working memory cognitive test dataset.\n\n\n\n\n\nsite: My website and blog post codes using Quarto Markdown\nmldc2020: Recommendation system and 7th place solution to the Mercado Libre Data Challenge 2020\nrstanbtm: Biterm Topic Model implementation in Stan\nqlm: Generate predictive SQL queries from linear models in R\ntophat: Scheduled shell script to fetch and save fantasy football data\n\n\n\n\n\nPod e Dev podcast episode, where I talk (in portuguese) about the challenges in pricing digital products and causal assumptions we made to overcome these challenges in our model at Hotmart. We also discuss good and bad use cases for large language models, as well as how models with 2 parameters can be as useful as models with 200 million parameters."
  },
  {
    "objectID": "index.html#cv",
    "href": "index.html#cv",
    "title": "  ",
    "section": "CV",
    "text": "CV"
  },
  {
    "objectID": "blog/fantasy-football/index.html",
    "href": "blog/fantasy-football/index.html",
    "title": "Picking a fantasy football team",
    "section": "",
    "text": "Cartola is a fantasy football league following the Brazilian Championship A Series.\nCartola offers a public API to access data for the current round. A couple of years ago, I created a script to automate data retrieval to a repository, which now hosts comprehensive historical data since 2022.\nIn this post, we will delve into the data for the 2022 season, formulate a mixed integer linear program to pick the optimal team, and present initial concepts for forecasting player scores using mixed effects linear models."
  },
  {
    "objectID": "blog/fantasy-football/index.html#the-game",
    "href": "blog/fantasy-football/index.html#the-game",
    "title": "Picking a fantasy football team",
    "section": "The game",
    "text": "The game\nWe begin the season with a budget of C$ 100, the game’s paper currency.\nEach round is preceded by a market session, where players are assigned a value. We are tasked with forming a team of 11 players plus a coach, all within our budget and adhering to a valid formation. A captain must be chosen from among the players, excluding the coach.\nThe market is available until the round starts. Players then earn scores based on their real-life match performances. Our team’s score is the aggregate of our players’ scores, with our captain’s score doubled in the 2022 season.\nFollowing the conclusion of the round, player values are recalibrated based on performance -— with increases for scores above their average and decreases for below-average performances. Our budget for the next round is our previous budget, plus the sum of our players’ value variations."
  },
  {
    "objectID": "blog/fantasy-football/index.html#data-wrangling",
    "href": "blog/fantasy-football/index.html#data-wrangling",
    "title": "Picking a fantasy football team",
    "section": "Data wrangling",
    "text": "Data wrangling\nLet’s talk about data structures: each round has a market, and each market is a list of players. A player is a structure like this:\nLet’s get the list of markets for 2022 and flatten it into a single DataFrame:\nNow, let’s focus on a specific player to illustrate our data while we wrangle it:\n\nFiltering participation\nPlayers will show up in the market for many rounds that they do not participate in. However, for our analysis, we are only interested in players that actually played a game in the round.\nEach player has a status field intended to indicate their participation in the round. However, this field is often inaccurate, likely due to the API data being updated before the round.\nOne solution is to keep only rows where there is an increase in the number of games the player has played:\nr”“” ## Imputing scores\nSimilarly, the player score field is often inaccurate, likely for the same reasons as the status field. Fortunately, the average field is reliable, allowing us to recover the score:\n\\[\n\\begin{align*}\n\\mathrm{Average}(\\mathbf{s}_{1:t})\n= \\frac{\\mathrm{Average}(\\mathbf{s}_{1:(t-1)}) + s_t}{2} \\\\\ns_t\n= 2\\mathrm{Average}(\\mathbf{s}_{1:t}) - \\mathrm{Average}(\\mathbf{s}_{1:(t-1)}),\n\\end{align*}\n\\]\nwhere \\(\\mathbf{s}\\) is the vector of scores for a given player across all rounds. ““”\n\n\nAdding fixtures\nLet’s fetch the list of fixtures to enrich our dataset. A fixture is an object like:\nLet’s consolidate these fixtures into a single DataFrame and then pivot them into a long format:\nFinally, let’s join this data to our dataset:\n\n\nAligning variables\nIn our subsequent analysis, the average field will exclude the score from the given round. Additionally, the appreciation field will be calculated in relation to the round’s score.\nr”“” # Team picking\nNow let’s solve the problem of picking the best team a given market. Let $ $ be the set of valid formations, then for each formation \\(f \\in\n\\mathcal{F}\\), solve:\n\\[\n\\begin{equation*} \\begin{array}{ll@{}ll}\n\\text{maximize} & \\displaystyle \\hat{\\mathbf{s}}^T \\mathbf{x}, & \\mathbf{x} \\in \\{\\mathbf{0}, \\mathbf{1}\\} \\\\\n\\text{subject to}\n& \\displaystyle \\mathbf{v}^T \\mathbf{x} \\leq b \\\\\n& \\displaystyle \\mathbf{P}^T \\mathbf{x} = f, \\\\\n\\end{array} \\end{equation*}\n\\]\nwhere\n\\(\\mathbf{x}\\) is a variable vector of player picks in the market; \\(\\hat{\\mathbf{s}}\\) is the vector of predicted player scores in the market; \\(b\\) is our available budget for that round; \\(\\mathbf{P}\\) is the matrix of dummy-encoded player formations in the market.\nFinally, take the solution with the highest objective. ““”\n\nclass Problem(BaseModel):\n    scores: List[float]\n    values: List[float]\n    budget: float\n    positions: List[List[int]]\n    formations: List[Formation]\n\n    def solve(self) -&gt; List[pulp.LpSolution]:\n        formations = [list(f.model_dump().values()) for f in self.formations]\n        problems = [self.construct(f) for f in formations]\n        [p.solve(pulp.COIN(msg=False)) for p in problems]\n        objectives = [p.objective.value() for p in problems]\n        best = np.argmax(np.array(objectives))\n        solution = problems[best]\n        variables = [v.value() for v in solution.variables()]\n        picks = np.array(variables)\n        return picks\n\n    def construct(self, formation: List[int]) -&gt; pulp.LpProblem:\n        n = len(self.scores)\n        m = len(formation)\n        problem = pulp.LpProblem(\"team_picking\", pulp.LpMaximize)\n        indexes = [\"pick_\" + str(i).zfill(len(str(n))) for i in range(n)]\n        picks = [pulp.LpVariable(i, cat=pulp.const.LpBinary) for i in indexes]\n        problem += pulp.lpDot(picks, self.scores)\n        problem += pulp.lpDot(picks, self.values) &lt;= self.budget\n        for i in range(m):\n            problem += pulp.lpDot(picks, self.positions[i]) == formation[i]\n        return problem\n\nr”“” ## Backtesting\nBy solving the team picking problem for all rounds, we can backtest our performance in the season. Before backtesting, let’s get the set of valid formations \\(\\mathcal{F}\\): ““”\nKnowing our formation constraints, we’re ready to backtest. Starting with a budget of C$ 100, for each round let’s:\n\nPredict each player’s score based on their performance on previous rounds;\nPick the team with the best total score;\nAdd the sum of the team player’s appreciation to our budget.\n\n\ndef backtest(\n    players: pl.DataFrame, predict: Callable, initial_budget: float = 100.0\n) -&gt; pl.DataFrame:\n    rounds = players.get_column(\"round\").max()\n    budget = [None] * rounds\n    teams = [None] * rounds\n    budget[0] = initial_budget\n    for round in range(rounds):\n        if round &gt; 0:\n            budget[round] = budget[round - 1] + appreciation\n        data = players.filter(pl.col(\"round\") &lt; round + 1)\n        candidates = players.filter(pl.col(\"round\") == round + 1)\n        candidates = predict(data, candidates)\n        problem = Problem(\n            scores=candidates.get_column(\"prediction\"),\n            values=candidates.get_column(\"value\"),\n            positions=candidates.get_column(\"position\").to_dummies(),\n            budget=budget[round],\n            formations=formations,\n        )\n        picks = problem.solve()\n        team = candidates.filter(picks == 1)\n        teams[round] = team\n        appreciation = team.get_column(\"appreciation\").sum()\n    teams = pl.concat(teams)\n    return teams\n\nBefore exploring predictions, we’ll begin with a few hypothetical backtests using actual observed scores for team selection. Backtesting this strategy, this is our team in the first round:\nAnd we can plot out cumulative performance during the season:\nThis might seem like a perfect campaign at first, but it’s possible that, early in the season, we didn’t have enough budget to pick the best scoring teams. To test this hypothesis, we backtest the same strategy with unlimited budget from the start:\nBoth runs are nearly identical, which is evidence that focusing on appreciation is not so important if we have accurate predictions for the scores. If we predict scores perfectly, we get a near perfect run.\nTo put our backtests into perspective, the 2022 season champion had a total score of 3434.37. This is very impressive and not very far from the near perfect run.\nr”“” # Score prediction\nFor each round, we must predict \\(\\hat{s}\\), the vector of score predictions, using data from previous rounds.\nHowever, during the first round, we don’t have any previous data to train our model. In this case, we need to include prior information. One way to do that would be to use data from previous seasons. However, we know a variable where this information is already encoded: the player value. Each season starts with players valued according to their past performance. Knowing this, all our models start with \\(\\hat{s} = v\\) in the first round.\nLet’s use Bambi (Capretto et al. 2022) and its default priors to fit our models. We won’t delve into convergence diagnostics, since we are more interested in the average of the predictive posteriors and the backtest itself is measure of the prediction quality.\n\nCapretto, Tomás, Camen Piho, Ravin Kumar, Jacob Westfall, Tal Yarkoni, and Osvaldo A Martin. 2022. “Bambi: A Simple Interface for Fitting Bayesian Linear Models in Python.” Journal of Statistical Software 103 (15): 1–29. https://doi.org/10.18637/jss.v103.i15.\n\nBailey, David H., Jonathan M. Borwein, Marcos Lopez de Prado, and Qiji Jim Zhu. 2013. “The Probability of Back-Test over-Fitting.” SSRN Electronic Journal. https://doi.org/10.2139/ssrn.2326253.\nOne question that arises here is: why not use non-parametric models such as gradient boosted trees or neural nets? After some experimentation, I concluded they are not a good fit for this problem: either because they assume independence between observations, or because they are too data hungry. Also, tuning these models for backtests might lead us into a rabbit hole (Bailey et al. 2013)."
  },
  {
    "objectID": "blog/fantasy-football/index.html#player-average",
    "href": "blog/fantasy-football/index.html#player-average",
    "title": "Picking a fantasy football team",
    "section": "Player average",
    "text": "Player average\n\\[\n\\begin{align*}\n\\mathbf{\\hat{s}} = \\mathbf{Z} \\mathbf{\\beta} \\\\\n\\mathbf{s} \\sim N(\\mathbf{\\hat{s}}, \\sigma),\n\\end{align*}\n\\]\nwhere \\(\\mathbf{Z}\\) is a dummy-encoded matrix of players; \\(\\mathbf{\\beta}\\) is a vector of parameters for each player.\nIn this model, \\(\\mathbf{\\beta}\\) is simply a vector of player averages. Let’s also consider that players that show up in the middle of the season have an average of zero before their first round. This will be our baseline model. ““”\nr”“” ## Player random effects\n\\[\n\\begin{align*}\n\\mathbf{\\hat{s}} = \\alpha + \\mathbf{Z} \\mathbf{b} \\\\\n\\mathbf{b} \\sim N(0, \\sigma_b),\n\\end{align*}\n\\]\nwhere \\(\\alpha\\) is an intercept and \\(\\mathbf{b}\\) is a vector of player random effects.\nThis model performs significantly better than the average model, possibly because of the partial pooling between the random effects, that pulls large effects towards the overall mean (Clark 2019). In our dataset, it’s common for players that played one or two games to have large averages by chance. ““”\n\nClark, Michael. 2019. “Michael Clark: Shrinkage in Mixed Effects Models.” https://m-clark.github.io/posts/2019-05-14-shrinkage-in-mixed-models/.\nr”“” ## Fixture mixed effects\n\\[\n\\mathbf{\\hat{s}} = \\alpha + \\mathbf{X} \\mathbf{\\beta} + \\mathbf{Z} \\mathbf{b},\n\\]\nwhere \\(\\mathbf{X}\\) is a matrix of the dummy-encoded fixture variables: the player team, whether they are playing at home, and their adversary team variables; \\(\\mathbf{\\beta}\\) is a vector of fixed effects.\nThis model brings more context to our predictions. It also provides a reasonable way to predict a new player, by setting their \\(b = 0\\) (the mean of the random effects). However, it does not improve significantly over our random effects model. ““”"
  },
  {
    "objectID": "blog/fantasy-football/index.html#conclusion",
    "href": "blog/fantasy-football/index.html#conclusion",
    "title": "Picking a fantasy football team",
    "section": "Conclusion",
    "text": "Conclusion\nWe developed a comprehensive framework for the fantasy football team picking problem. There are more ideas we could explore to improve our chances of winning:\n\nenriching our data and models with player scouts;\nincluding more information in our priors;\ntesting strategies that balance predicted score and appreciation;\nfurther model diagnostics.\n\nHowever, I suppose expert human player predictions have a certain edge over those of hobbyist statistical models in fantasy leagues, due to the fact that there are all sorts of relevant data unavailable in public datasets.\nAt least, this seems to be the case for brazilian soccer, also known as “a little box of surprises”."
  },
  {
    "objectID": "blog/non-monotonic/index.html",
    "href": "blog/non-monotonic/index.html",
    "title": "Additive aging curve",
    "section": "",
    "text": "Warning\n\n\n\nThis post is a draft.\nRecently, I helped design an experiment measuring a binary response against a continuous variable. If the user abandoned their cart at time zero, then we delayed for a variable number of minutes before reminding them to finish their purchase. The delay has a non-monotonic relationship to the response: as the delay increases, so does the purchase rate; then the rate peaks; and finally it decreases.\nCausally, we may decompose this process into two: as the delay increases, the user 1) becomes more available for and 2) loses interest in purchasing the product. This is a common phenomena in different time-based scenarios. In sports, the “aging curve” refers to how a player’s performance increases with age, then decreases. As the player gets older, they get 1) better at the sport and 2) physically weaker.\nAndrew Gelman wrote about this a couple of times in his blog: see his posts from 2018 and 2023, where Gelman suggests modeling these processes using an additive function like:\n\\[g(x) = g_1(x) + g_2(x),\\]\nwhere\n\\(g_1(x)\\) is a monotonically increasing function with a right asymptote; and\n\\(g_2(x)\\) is a monotonically decreasing function with a left asymptote.\nIn this post, we’ll analyse an experimental dataset by fitting and comparing three different models: a non-parametric bootstrap, a semi-parametric spline and a fully parametric decomposable curve like \\(g(x)\\)."
  },
  {
    "objectID": "blog/non-monotonic/index.html#the-digit-span-test",
    "href": "blog/non-monotonic/index.html#the-digit-span-test",
    "title": "Additive aging curve",
    "section": "The Digit Span test",
    "text": "The Digit Span test\nThe motivation for Gelman’s 2018 post was a study relating age to peak cognitive functioning (Hartshorne and Germine 2015). According to the study, one of their experiments was a large scale online experimentation platform:\n\nHartshorne, Joshua K., and Laura T. Germine. 2015. “When Does Cognitive Functioning Peak? The Asynchronous Rise and Fall of Different Cognitive Abilities Across the Life Span.” Psychological Science 26 (4): 433–43. https://doi.org/10.1177/0956797614567339.\n\nParticipants in Experiment 2 (N = 10,394; age range = 10–69 years old) […] were visitors to TestMyBrain.org, who took part in experiments in order to contribute to scientific research and in exchange for performance-related feedback. […] We continued data collection for each experiment for approximately 1 year, sufficient to obtain around 10,000 participants, which allowed fine-grained age-of-peak-performance analysis.\n\nThe dataset for Experiment 2 is available online (Germine and Hartshorne 2016) and includes results of the Digit Span verbal working memory test, part of the Wechsler Adult Intelligence Scale (WAIS) and Wechsler Memory Scale (WMS) supertests. In the Digit Span test, subjects must repeat lists of digits, either in the same or reversed order.\n\nGermine, Laura, and Joshua K Hartshorne. 2016. “Hartshorne & Germine (2015) When Does Cognitive Functioning Peak?” OSF. osf.io/f2saj.\nLet’s plot the relationship between age and Digit Span performance:\n\n\n\n\n\n\n\n\n\nVisually, it’s still unclear if this relationship follows an aging curve, but we’ll get back to this matter in the next section."
  },
  {
    "objectID": "blog/non-monotonic/index.html#bootstrap-estimates",
    "href": "blog/non-monotonic/index.html#bootstrap-estimates",
    "title": "Additive aging curve",
    "section": "Bootstrap estimates",
    "text": "Bootstrap estimates\nIn the original paper, the authors describe a bootstrap resampling procedure to estimate the distribution of ages of peak performance:\n\nEstimates and standard errors for age of peak performance were calculated using a bootstrap resampling procedure identical to the one used in Experiment 1 but applied to raw performance data. To dampen noise, we smoothed means for each age using a moving 3-year window prior to identifying age of peak performance in each sample. Other methods of dampening noise provide similar results.\n\nLet’s decompose this method (as I understand it) into steps:\n\nWith replacement, sample \\(n\\) observations from the dataset;\nCalculate the mean performance for each sample and age;\nRepeat steps 1 and 2 \\(m\\) times to get multiple samples;\nSort each sample by age and smooth age means using a 3-year rolling average;\nFind the age of peak performance for each sample.\n\n\nimport polars as pl\n\nn = experiment.height\nm = 10000\nnm = n * m\nseed = 37\nsamples = (\n    experiment.sample(nm, with_replacement=True, seed=seed)\n    .with_columns(sample=pl.arange(1, nm + 1) % m)\n    .group_by(\"sample\", \"age\")\n    .agg(mean=pl.col(\"y\").mean())\n    .sort(\"sample\", \"age\")\n    .with_columns(smoothed_mean=pl.col(\"mean\").rolling_mean(3).over(\"sample\"))\n)\npeak_performance = samples.group_by(\"sample\").agg(\n    age=pl.col(\"age\").get(pl.col(\"smoothed_mean\").arg_max())\n)\n\nThis yields the following bootstrap distribution of ages of peak performance:\n\n\n\n\n\n\n\n\n\nThis distribution suggests two important things:\n\nThe most probable age of peak performance is, by far, 33;\nThere is a non-negligible probability that the age of peak performance happens in the early 20s, but a negligible probability that it happens in the late 20s.\n\nThing 2 certainly deserves attention. This is possibly caused by a confound variable or some measuring error, but I won’t investigate this any further. Instead, let’s get back to estimating curves. We will use the samples from step 4 to summarize the distribution of mean performances. For each age, we calculate the mean and 90% interquantile range, yielding a nonparametric curve:\n\n\n\n\n\n\n\n\n\nThis figure is analogue to figure … in the paper. Since this is an entirely empirical curve, there isn’t much to interpret here (maybe unitary changes?). However, the curve shape indicates an aging-curve-likeness."
  },
  {
    "objectID": "blog/non-monotonic/index.html#penalized-splines",
    "href": "blog/non-monotonic/index.html#penalized-splines",
    "title": "Additive aging curve",
    "section": "Penalized splines",
    "text": "Penalized splines\nSplines are wiggly curves…\n\\[\n\\begin{align}\ng(x) &= \\alpha + Z \\bf{b} \\\\\ny &\\sim \\mathrm{Normal}(g(x), \\sigma) \\\\\n\\alpha &\\sim \\mathrm{Student}(3, 0, 0.1) \\\\\n\\sigma &\\sim \\mathrm{HalfCauchy}(1)\n\\end{align}\n\\]\nPolynomials have runge swings…\nWe could make assumptions about the data generating process to help us pick the number of knots. Instead, let’s pick an arbitrary large number of knots (say, 15) and let the model itself learn how wiggly the curve should be.\n\\[\n\\begin{align}\nb &= \\tau \\bf{z} \\\\\n\\tau &\\sim \\mathrm{HalfCauchy}(1) \\\\\n\\bf{z} &\\sim \\mathrm{Normal}(0, 1)\n\\end{align}\n\\]\nhttps://www.pymc.io/projects/examples/en/latest/howto/spline.html\nhttps://www.tjmahr.com/random-effects-penalized-splines-same-thing/\nhttps://elevanth.org/blog/2017/09/07/metamorphosis-multilevel-model/\n\nimport pymc as pm\n\nwith pm.Model() as spline:\n    Z = pm.ConstantData(\"Z\", Z)\n    α = pm.StudentT(\"α\", 3, 0, sigma=0.1)\n    τ = pm.HalfCauchy(\"τ\", 1)\n    z = pm.Normal(\"z\", 0, 1, size=Z.shape[1])\n    b = pm.Deterministic(\"b\", τ * z)\n    μ = pm.Deterministic(\"μ\", α + pm.math.dot(Z, b.T))\n    σ = pm.HalfCauchy(\"σ\", 1)\n    pm.Normal(\"y\", μ, σ, observed=y)\n\n\n\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nSequential sampling (4 chains in 1 job)\nNUTS: [α, τ, z, σ]\nSampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 47 seconds.\n\n\n::: {#cell-spline-plot .cell 0=‘s’ 1=‘p’ 2=‘l’ 3=‘i’ 4=‘n’ 5=‘e’ 6=‘-’ 7=‘p’ 8=‘l’ 9=‘o’ 10=‘t’ execution_count=9}\n\n\n\n\n\n\n\n:::\nSplines are good interpolation tools…\n“when, where and how things change”… https://www.youtube.com/watch?v=Zxokd_Eqrcg&t=506s\nHowever, it’s not a good idea to extrapolate…"
  },
  {
    "objectID": "blog/non-monotonic/index.html#two-component-function",
    "href": "blog/non-monotonic/index.html#two-component-function",
    "title": "Additive aging curve",
    "section": "Two component function",
    "text": "Two component function\nAll intervals are 80% credibility…\n\\[\n\\begin{align}\ng_1(x) &= \\alpha_1 + \\beta_1 \\exp(-\\lambda_1 x) \\\\\ng_2(x) &= \\alpha_2 + \\beta_2 (1 - \\exp(-\\lambda_2 x)) \\\\\ng(x) &= g_1(x) + g_2(x) \\\\\n     &= \\alpha + \\beta_1 \\exp(-\\lambda_1 x) + \\beta_2 (1 - \\exp(-\\lambda_2 x))\n\\end{align}\n\\]\n\\[\n\\begin{align}\ny &\\sim \\mathrm{Normal}(g(x), \\sigma) \\\\\n\\alpha &\\sim \\mathrm{Normal}(0, 2) \\\\\n\\lambda &\\sim \\mathrm{Exponential}(0.01) \\\\\n\\sigma &\\sim \\mathrm{Exponential}(1) \\\\\n\\end{align}\n\\]\n\nimport pymc as pm\n\n\ndef g(x):\n    y = α[0] * pm.math.exp(-λ[0] * x) + α[1] + α[2] * (1 - pm.math.exp(-λ[1] * x))\n    return y\n\n\nwith pm.Model() as model:\n    x = pm.ConstantData(\"x\", x)\n    α = pm.Normal(\"alpha\", 0, 2, size=3)\n    λ = pm.HalfNormal(\"lambda\", 0.01, size=2)\n    μ = pm.Deterministic(\"mu\", g(x))\n    σ = pm.HalfNormal(\"sigma\", 1)\n    pm.Normal(\"observed\", mu=μ, sigma=σ, observed=y)\n\n\n\n\n\n\n\n\n\n\n\\[d/dx g(x) = a_2 b_2 e^(b_2 (-x)) - a_1 b_1 e^(b_1 (-x))\\] \\[x = \\frac{\\log(\\frac{a_1 b_1}{a_2 b_2})}{b_1 - b_2}\\]"
  },
  {
    "objectID": "index.html#footnotes",
    "href": "index.html#footnotes",
    "title": "  ",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n↩︎"
  },
  {
    "objectID": "index.html#about",
    "href": "index.html#about",
    "title": "Home",
    "section": "",
    "text": "Hello! I’m Luís Assunção1 and this is my website.\n1 I’m a statistician and data scientist – my goal is to help people make better decisions under uncertainty. To achieve this, I design experiments, infer causal relationships, model probabilities, and more.\nI also enjoy hiking, music and woodworking. I live with my partner and our ginger cat2 in Belo Horizonte, Brazil.\n2"
  },
  {
    "objectID": "index.html#section",
    "href": "index.html#section",
    "title": "Home",
    "section": "",
    "text": "/"
  }
]