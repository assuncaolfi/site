{
  "hash": "ff455acec4a5c1e4b65cb1b849ba3ea5",
  "result": {
    "markdown": "---\ntitle: Additive aging curve\ndate: today\n---\n\n::: {.callout-warning}\nThis post is a work in progress.\n:::\n\nRecently, I helped design an experiment measuring a binary response against a\ncontinuous variable. If the user abandoned their cart at time zero, then we\ndelayed for a variable number of minutes before reminding them to finish their\npurchase. The delay has a non-monotonic relationship to the response:\nas the delay increases, so does the purchase rate; then the rate peaks; and finally it decreases.\n\nCausally, we may decompose this process into two: as the delay increases,\nthe user 1) becomes more available for and 2) loses interest in purchasing\nthe product. This is a common phenomena in different time-based scenarios. In\nsports, the \"aging curve\" refers to how a player's performance increases with\nage, then decreases. As the player gets older, they get 1) better at the sport\nand 2) physically weaker.\n\nAndrew Gelman wrote about this a couple of times in his blog: see his posts\nfrom [2018](https://statmodeling.stat.columbia.edu/2018/09/07/bothered-non-monotonicity-heres-one-quick-trick-make-happy/)\nand [2023](https://statmodeling.stat.columbia.edu/2023/01/01/how-to-model-a-non-monotonic-relation/), \nwhere Gelman suggests modeling these processes using an additive function like:\n\n$$g(x) = g_1(x) + g_2(x),$$\n\nwhere  \n$g_1(x)$ is a monotonically increasing function with a right asymptote; and  \n$g_2(x)$ is a monotonically decreasing function with a left asymptote.\n\nIn this post, we'll analyse an experimental dataset by fitting and comparing\nthree different models: a non-parametric bootstrap, a semi-parametric spline\nand a fully parametric decomposable curve like $g(x)$.\n\n## The Digit Span test\n\nThe motivation for Gelman's 2018 post was a study relating age to peak cognitive \nfunctioning [@Hartshorne2015]. According to the study, one of their experiments\nwas a large scale online experimentation platform:\n\n> Participants in Experiment 2 (N = 10,394; age range = 10–69 years old) [...]\n> were visitors to TestMyBrain.org, who took part in experiments in order to\n> contribute to scientific research and in exchange for performance-related\n> feedback. [...] We continued data collection for each experiment for approximately\n> 1 year, sufficient to obtain around 10,000 participants, which allowed fine-grained\n> age-of-peak-performance analysis.\n\nThe dataset for Experiment 2 is available online [@Germine_Hartshorne_2016] and \nincludes results of the Digit Span verbal working memory test, part of the Wechsler \nAdult Intelligence Scale (WAIS) and Wechsler Memory Scale (WMS) supertests. In the \nDigit Span test, subjects must repeat lists of digits, either in the same or reversed \norder.\n\n\n\nLet's plot the relationship between age and Digit Span performance:\n\n::: {#cell-digit-span-plot .cell execution_count=2}\n\n::: {.cell-output .cell-output-display execution_count=2}\n![](index_files/figure-html/digit-span-plot-output-1.png){#digit-span-plot width=599 height=445}\n:::\n:::\n\n\nVisually, it's still unclear if this relationship follows an aging curve, but we'll get back to this matter in the next section.\n\n## Bootstrap estimates\n\nIn the original paper, the authors describe a bootstrap resampling procedure\nto estimate the distribution of ages of peak performance:\n\n> Estimates and standard errors for age of peak performance were calculated using\n> a bootstrap resampling procedure identical to the one used in Experiment 1\n> but applied to raw performance data. To dampen noise, we smoothed means for each\n> age using a moving 3-year window prior to identifying age of peak performance\n> in each sample. Other methods of dampening noise provide similar results.\n\nLet's decompose this method (as I understand it) into steps:\n\n1. With replacement, sample $n$ observations from the dataset;\n2. Calculate the mean performance for each sample and age;\n3. Repeat steps 1 and 2 $m$ times to get multiple samples;\n4. Sort each sample by age and smooth age means using a 3-year rolling average;\n5. Find the age of peak performance for each sample.\n\n::: {#bootstrap .cell filename='Bootstrap Model' execution_count=3}\n``` {.python .cell-code}\nn = experiment.height\nm = 10000\nnm = n * m\nseed = 37\nsamples = (\n    experiment.sample(nm, with_replacement=True, seed=seed)\n    .with_columns(sample=pl.arange(1, nm + 1) % m)\n    .group_by(\"sample\", \"age\")\n    .agg(mean=pl.col(\"y\").mean())\n    .sort(\"sample\", \"age\")\n    .with_columns(smoothed_mean=pl.col(\"mean\").rolling_mean(3).over(\"sample\"))\n)\npeak_performance = samples.group_by(\"sample\").agg(\n    age=pl.col(\"age\").get(pl.col(\"smoothed_mean\").arg_max())\n)\n```\n:::\n\n\nThis yields the following bootstrap distribution of ages of peak performance:\n\n::: {#cell-bootstrap-distribution .cell execution_count=4}\n\n::: {.cell-output .cell-output-display execution_count=4}\n![](index_files/figure-html/bootstrap-distribution-output-1.png){#bootstrap-distribution width=599 height=445}\n:::\n:::\n\n\nThis distribution suggests two important things:\n\n1. The most probable age of peak performance is, by far, 33;\n2. There is a non-negligible probability that the age of peak performance happens in the early 20s, but a negligible probability that it happens in the late 20s.\n\nThing 2 certainly deserves attention. This is possibly caused by a confound variable or some measuring error, but I won't investigate this any further. Instead, let's get back to estimating curves. We will use the samples from step 4 to summarize the distribution of mean performances. For each age, we calculate the mean and 90% interquantile range, yielding a nonparametric curve:\n\n::: {#cell-bootstrap-curve .cell execution_count=5}\n\n::: {.cell-output .cell-output-display execution_count=5}\n![](index_files/figure-html/bootstrap-curve-output-1.png){#bootstrap-curve width=599 height=445}\n:::\n:::\n\n\nThis figure is analogue to figure ... in the paper. Since this is an entirely empirical curve, there isn't much to interpret here (maybe unitary changes?). However, the curve shape indicates an aging-curve-likeness.\n\n## Penalized splines\n\nSplines are wiggly curves...\n\n$$\n\\begin{align}\ng(x) &= \\alpha + Z \\bf{b} \\\\\ny &\\sim \\mathrm{Normal}(g(x), \\sigma) \\\\\n\\alpha &\\sim \\mathrm{Student}(3, 0, 0.1) \\\\\n\\sigma &\\sim \\mathrm{HalfCauchy}(1)\n\\end{align}\n$$\n\nPolynomials have runge swings...\n\nWe could make assumptions about the data generating process to help us pick the number of knots. Instead, let's pick an arbitrary large number of knots (say, 15) and let the model itself learn how wiggly the curve should be.\n\n$$\n\\begin{align}\nb &= \\tau \\bf{z} \\\\\n\\tau &\\sim \\mathrm{HalfCauchy}(1) \\\\\n\\bf{z} &\\sim \\mathrm{Normal}(0, 1)\n\\end{align}\n$$\n\nhttps://www.pymc.io/projects/examples/en/latest/howto/spline.html  \nhttps://www.tjmahr.com/random-effects-penalized-splines-same-thing/  \nhttps://elevanth.org/blog/2017/09/07/metamorphosis-multilevel-model/  \n\n\n\n::: {#spline-model .cell filename='Spline Model' execution_count=7}\n``` {.python .cell-code}\nimport pymc as pm\n\nwith pm.Model() as spline:\n    Z = pm.ConstantData(\"Z\", Z)\n    α = pm.StudentT(\"α\", 3, 0, sigma=0.1)\n    τ = pm.HalfCauchy(\"τ\", 1)\n    z = pm.Normal(\"z\", 0, 1, size=Z.shape[1])\n    b = pm.Deterministic(\"b\", τ * z)\n    μ = pm.Deterministic(\"μ\", α + pm.math.dot(Z, b.T))\n    σ = pm.HalfCauchy(\"σ\", 1)\n    pm.Normal(\"y\", μ, σ, observed=y)\n```\n:::\n\n\n::: {#cell-spline-graph .cell execution_count=8}\n\n::: {.cell-output .cell-output-display execution_count=8}\n![](index_files/figure-html/spline-graph-output-1.svg){#spline-graph}\n:::\n:::\n\n\n\n\n::: {#cell-spline-plot .cell 0='s' 1='p' 2='l' 3='i' 4='n' 5='e' 6='-' 7='p' 8='l' 9='o' 10='t' execution_count=10}\n\n::: {.cell-output .cell-output-display execution_count=10}\n![](index_files/figure-html/spline-plot-output-1.png){#spline-plot width=599 height=445}\n:::\n:::\n\n\n## Two component function\n\nAll intervals are 80% credibility...\n\n$$\n\\begin{align}\ng_1(x) &= \\alpha_1 + \\beta_1 \\exp(-\\lambda_1 x) \\\\\ng_2(x) &= \\alpha_2 + \\beta_2 (1 - \\exp(-\\lambda_2 x)) \\\\\ng(x) &= g_1(x) + g_2(x) \\\\\n     &= \\alpha + \\beta_1 \\exp(-\\lambda_1 x) + \\beta_2 (1 - \\exp(-\\lambda_2 x))\n\\end{align}\n$$\n\n$$\n\\begin{align}\ny &\\sim \\mathrm{Normal}(g(x), \\sigma) \\\\\n\\alpha &\\sim \\mathrm{Normal}(0, 2) \\\\\n\\lambda &\\sim \\mathrm{Exponential}(0.01) \\\\\n\\sigma &\\sim \\mathrm{Exponential}(1) \\\\\n\\end{align}\n$$\n\n::: {#cell-decomposable-models-additive .cell filename='Additive Model' execution_count=11}\n``` {.python .cell-code}\ndef g(x):\n    y = α[0] * pm.math.exp(-λ[0] * x) + α[1] + α[2] * (1 - pm.math.exp(-λ[1] * x))\n    return y\n\n\nwith pm.Model() as model:\n    x = pm.ConstantData(\"x\", x)\n    α = pm.Normal(\"alpha\", 0, 2, size=3)\n    λ = pm.HalfNormal(\"lambda\", 0.01, size=2)\n    μ = pm.Deterministic(\"mu\", g(x))\n    σ = pm.HalfNormal(\"sigma\", 1)\n    pm.Normal(\"observed\", mu=μ, sigma=σ, observed=y)\n    prediction = pm.Deterministic(\"prediction\", g(x_range))\n    peak = pm.Deterministic(\n        \"peak\", pm.math.log(α[0] * λ[0] / (α[2] * λ[1])) / (λ[0] - λ[1])\n    )\n    slope = pm.Deterministic(\n        \"slope\",\n        pm.math.log(α[0] * λ[0] ** 2 / (α[2] * λ[1] ** 2)) / (λ[0] - λ[1]),\n    )\n    # traces = pm.sample_prior_predictive(samples=2000, random_seed=seed)\n    traces = pm.sample(progressbar=False, random_seed=seed)\n\n# add_model(fig, traces.prior.prediction, \"Prior Curves (80% Coverage)\")\nadd_model(fig, traces.posterior.prediction, \"Posterior Curves (80% Coverage)\")\n```\n\n::: {.cell-output .cell-output-display execution_count=11}\n![](index_files/figure-html/decomposable-models-additive-output-1.png){#decomposable-models-additive width=599 height=445}\n:::\n:::\n\n\n$$d/dx g(x) = a_2 b_2 e^(b_2 (-x)) - a_1 b_1 e^(b_1 (-x))$$\n$$x = \\frac{\\log(\\frac{a_1 b_1}{a_2 b_2})}{b_1 - b_2}$$\n\n::: {#decomposable-models-peak .cell execution_count=12}\n\n::: {#decomposable-models-peak-1 .cell-output .cell-output-display execution_count=12}\n```\narray([<Axes: title={'center': 'peak'}>,\n       <Axes: title={'center': 'slope'}>], dtype=object)\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/decomposable-models-peak-output-2.png){#decomposable-models-peak-2 width=1217 height=496}\n:::\n:::\n\n\n",
    "supporting": [
      "index_files/figure-html"
    ],
    "filters": [],
    "includes": {}
  }
}
