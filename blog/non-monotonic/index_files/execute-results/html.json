{
  "hash": "5220c3b5c6022b530a1e29cb6867f3cd",
  "result": {
    "markdown": "---\ntitle: Decomposable non-monotonic models\ndate: 2023-11-17\ncategories: [causal, cognitive, modeling]\n---\n\n_This post is a work in progress._\n\nRecently, I helped design an experiment measuring a binary response against a\ncontinuous delay time. If the user did not do the thing at time zero, then we\ndelayed for a variable number of minutes before reminding them to do it. This\ndelay had a non-monotonic relationship to the response: as the delay increased,\nthe response responded differently. Initially, the response increased; then it\npeaked; and finally it decreased.\n\nCausally, we may decompose this process into two: assuming the user forgot to/\ncould not do the thing at the time, as the delay increases, they  1) become\nmore available for and 2) lose interest in doing the thing. This is a common\nphenomena in different time-based scenarios. In sports, the \"aging curve\" refers\nto how a player's performance increases with age, then decreases. As the player\ngets older, they get 1) better at the sport and 2) physically weaker.\n\nAndrew Gelman wrote about this a couple of times in his blog: see \n[this post from 2018](https://statmodeling.stat.columbia.edu/2018/09/07/bothered-non-monotonicity-heres-one-quick-trick-make-happy/)\nand [this one from 2023](https://statmodeling.stat.columbia.edu/2023/01/01/how-to-model-a-non-monotonic-relation/),\nas well as their comments, which also informed this post. Gelman proposed that\nwe should model these processes like this:\n\n$$g(x) = g_1(x) + g_2(x),$$\n\nwhere  \n$g_1(x)$ is a monotonically increasing function with a right asymptote; and  \n$g_2(x)$ is a monotonically decreasing function with a left asymptote.\n\nIn this post, we will go over some of these models and test them on a dataset.\n\n## Mind-in-Eyes\n\nThe dataset for the study is available... [@Hartshorne2015]\n\nWe could control for other variables, such as the computer type (desk or\nlaptop), but let's assume there are no confounding effects at play here.\n\n\n\n::: {#cell-mind-in-eyes-plot .cell execution_count=3}\n\n::: {.cell-output .cell-output-display execution_count=2}\n![](index_files/figure-html/mind-in-eyes-plot-output-1.png){#mind-in-eyes-plot width=599 height=445}\n:::\n:::\n\n\n## Empirical models\n\nAfter struggling with splines,\nSplines, Gaussian Processes...\n\n## Decomposable models\n\nSome commenters on Andrew's blog...\n\nAll intervals are 80% credibility...\n\n$$\n\\begin{align}\ng(x) = g_1(x) + g_2(x) \\\\\ny \\sim \\mathrm{Normal}(g(x), \\sigma) \\\\\n\\sigma \\sim \\mathrm{HalfNormal}(1)\n\\end{align}\n$$\n\n### Laurent polynomial\n\nDegree = 2, order = -1.\n\n$$\ng(x) = ax^{-1} + bx + cx^2\n$$\n\n### Siler\n\n$$\ng(x) = \\alpha_1 \\exp(-\\lambda_1 x) + \\alpha_2 + \\alpha_3 \\exp(\\lambda_2 x)\n$$\n\nWith priors:\n\n$$\n\\begin{align}\n\\alpha \\sim \\mathrm{Normal}(0, 2) \\\\\n\\lambda \\sim \\mathrm{HalfNormal}(0.01) \\\\\n\\end{align}\n$$\n\n::: {#cell-parametric-models-siler-prior-predictive .cell execution_count=4}\n\n::: {.cell-output .cell-output-display execution_count=3}\n![](index_files/figure-html/parametric-models-siler-prior-predictive-output-1.png){#parametric-models-siler-prior-predictive width=599 height=445}\n:::\n:::\n\n\n::: {#cell-parametric-models-siler-posterior-predictive .cell execution_count=5}\n\n::: {.cell-output .cell-output-display execution_count=4}\n![](index_files/figure-html/parametric-models-siler-posterior-predictive-output-1.png){#parametric-models-siler-posterior-predictive width=599 height=445}\n:::\n:::\n\n\n### McElreath\n\n$$\ng(x) = \\exp(-ax) (1 - exp(-bx))^c\n$$\n\n## Comparison\n\nLet's compare using LOO...\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}