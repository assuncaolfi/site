[
  {
    "objectID": "cv.html",
    "href": "cv.html",
    "title": "Curriculum Vitae",
    "section": "",
    "text": "Federal University of Minas Gerais (UFMG) | Belo Horizonte, MG - Brazil | 2017 - 2021\n\nResearch and reproducible monograph (in portuguese with an abstract in english) on exponential random graphs applied to epidemiology\nCo-author of Frequency and burden of neurological manifestations upon hospital presentation in COVID-19 patients: Findings from a large Brazilian cohort"
  },
  {
    "objectID": "cv.html#education",
    "href": "cv.html#education",
    "title": "Curriculum Vitae",
    "section": "",
    "text": "Federal University of Minas Gerais (UFMG) | Belo Horizonte, MG - Brazil | 2017 - 2021\n\nResearch and reproducible monograph (in portuguese with an abstract in english) on exponential random graphs applied to epidemiology\nCo-author of Frequency and burden of neurological manifestations upon hospital presentation in COVID-19 patients: Findings from a large Brazilian cohort"
  },
  {
    "objectID": "cv.html#employment",
    "href": "cv.html#employment",
    "title": "Curriculum Vitae",
    "section": "Employment",
    "text": "Employment\n\nHotmart\nStaff Data Scientist | April 2020 - present\nTo help creators live their passions, I worked in:\n\nDeveloping an in-house AB testing framework with optional stopping\nConsulting for and developing randomized controlled trials\nEstimating causal effects in non-randomized experiments\nHierarchical modeling of pricing elasticity for digital products\nHidden state modeling of evergreen vs launching sales strategies\nTopification and latent variable modeling for creators and products\n\n\n\nOper\nData Scientist | Oct 2018 - March 2020\nFor companies such as AB InBev and GTB to understand and make decisions from data, I worked in:\n\nSpatial modeling of pricing elasticity for beverages\nMulti-touchpoint attribution modeling\n\n\n\nIRIS\nIntern | 2015 - 2017\nTo defend and promote public policies that advance human rights in the digital matters, I worked in:\n\nCollecting, wrangling and describing data"
  },
  {
    "objectID": "cv.html#examples",
    "href": "cv.html#examples",
    "title": "Curriculum Vitae",
    "section": "Examples",
    "text": "Examples\n\nBlog\nSome blog posts:\n\nPicking a fantasy football team: In this post, I delve into the data for the 2022 season of a brazilian fantasy football league; formulate a mixed integer linear program to pick the optimal team; and present initial concepts for forecasting player scores using mixed effects linear models.\nParametric non-monotonic models: In this post, I analyse some function alternatives for modeling non-monotonic relationships with interpretable parameters, and compare these models using a a cognitive decline dataset.\n\nClick the  CODE link on the upper right of any post to see its full code. Common tools used in these posts: Python, polars, pymc and pulp.\n\n\nRepositories\n\nsite: My website and blog post codes\nmldc2020: 7th place solution to the Mercado Libre Data Challenge 2020\nrstanbtm: Biterm Topic Model implementation in Stan\nqlm: Generate predictive SQL queries from linear models in R\ntophat: Scheduled script to fetch and save fantasy football data\n\n\n\nOther stuff\n\nPod e Dev podcast episode, where I talk (in portuguese) about the challenges in pricing digital products and causal assumptions we made to overcome these challenges in our model at Hotmart. We also discuss good and bad use cases for large language models, as well as how models with 2 parameters can be as useful as models with 200 million parameters."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About",
    "section": "",
    "text": "Hello! My name is Luís Assunção and I’m a Data Scientist.\nI help people make better decisions under uncertainty through experimentation, causal inference and probabilistic modeling.\nI also enjoy hiking, music and woodworking. I live with my partner and our ginger cat in Belo Horizonte, Brazil."
  },
  {
    "objectID": "blog/non-monotonic/index.html",
    "href": "blog/non-monotonic/index.html",
    "title": "Decomposable non-monotonic models",
    "section": "",
    "text": "This post is a work in progress.\nRecently, I helped design an experiment measuring a binary response against a continuous delay time. If the user did not do the thing at time zero, then we delayed for a variable number of minutes before reminding them to do it. This delay had a non-monotonic relationship to the response: as the delay increased, the response responded differently. Initially, the response increased; then it peaked; and finally it decreased.\nCausally, we may decompose this process into two: assuming the user forgot to/ could not do the thing at the time, as the delay increases, they 1) become more available for and 2) lose interest in doing the thing. This is a common phenomena in different time-based scenarios. In sports, the “aging curve” refers to how a player’s performance increases with age, then decreases. As the player gets older, they get 1) better at the sport and 2) physically weaker.\nAndrew Gelman wrote about this a couple of times in his blog: see this post from 2018 and this one from 2023, as well as their comments, which also informed this post. Gelman proposed that we should model these processes like this:\n\\[g(x) = g_1(x) + g_2(x),\\]\nwhere\n\\(g_1(x)\\) is a monotonically increasing function with a right asymptote; and\n\\(g_2(x)\\) is a monotonically decreasing function with a left asymptote.\nIn this post, we will go over some of these models and test them on a dataset."
  },
  {
    "objectID": "blog/non-monotonic/index.html#mind-in-eyes",
    "href": "blog/non-monotonic/index.html#mind-in-eyes",
    "title": "Decomposable non-monotonic models",
    "section": "Mind-in-Eyes",
    "text": "Mind-in-Eyes\nThe dataset for the study is available… (Hartshorne and Germine 2015)\nWe could control for other variables, such as the computer type (desk or laptop), but let’s assume there are no confounding effects at play here."
  },
  {
    "objectID": "blog/non-monotonic/index.html#empirical-models",
    "href": "blog/non-monotonic/index.html#empirical-models",
    "title": "Decomposable non-monotonic models",
    "section": "Empirical models",
    "text": "Empirical models\nAfter struggling with splines, Splines, Gaussian Processes…"
  },
  {
    "objectID": "blog/non-monotonic/index.html#decomposable-models",
    "href": "blog/non-monotonic/index.html#decomposable-models",
    "title": "Decomposable non-monotonic models",
    "section": "Decomposable models",
    "text": "Decomposable models\nSome commenters on Andrew’s blog…\nAll intervals are 80% credibility…\n\\[\n\\begin{align}\ng(x) = g_1(x) + g_2(x) \\\\\ny \\sim \\mathrm{Normal}(g(x), \\sigma) \\\\\n\\sigma \\sim \\mathrm{HalfNormal}(1)\n\\end{align}\n\\]\n\nLaurent polynomial\nDegree = 2, order = -1.\n\\[\ng(x) = ax^{-1} + bx + cx^2\n\\]\n\n\nSiler\n\\[\ng(x) = \\alpha_1 \\exp(-\\lambda_1 x) + \\alpha_2 + \\alpha_3 \\exp(\\lambda_2 x)\n\\]\nWith priors:\n\\[\n\\begin{align}\n\\alpha \\sim \\mathrm{Normal}(0, 2) \\\\\n\\lambda \\sim \\mathrm{HalfNormal}(0.01) \\\\\n\\end{align}\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMcElreath\n\\[\ng(x) = \\exp(-ax) (1 - exp(-bx))^c\n\\]"
  },
  {
    "objectID": "blog/non-monotonic/index.html#comparison",
    "href": "blog/non-monotonic/index.html#comparison",
    "title": "Decomposable non-monotonic models",
    "section": "Comparison",
    "text": "Comparison\nLet’s compare using LOO…"
  },
  {
    "objectID": "blog/fantasy-football/index.html",
    "href": "blog/fantasy-football/index.html",
    "title": "Picking a fantasy football team",
    "section": "",
    "text": "Cartola is a fantasy football league following the Brazilian Championship A Series.\nCartola offers a public API to access data for the current round. A couple of years ago, I created a script to automate data retrieval to a repository, which now hosts comprehensive historical data since 2022.\nIn this post, we will delve into the data for the 2022 season, formulate a mixed integer linear program to pick the optimal team, and present initial concepts for forecasting player scores using mixed effects linear models."
  },
  {
    "objectID": "blog/fantasy-football/index.html#the-game",
    "href": "blog/fantasy-football/index.html#the-game",
    "title": "Picking a fantasy football team",
    "section": "The game",
    "text": "The game\nWe begin the season with a budget of C$ 100, the game’s paper currency.\nEach round is preceded by a market session, where players are assigned a value. We are tasked with forming a team of 11 players plus a coach, all within our budget and adhering to a valid formation. A captain must be chosen from among the players, excluding the coach.\nThe market is available until the round starts. Players then earn scores based on their real-life match performances. Our team’s score is the aggregate of our players’ scores, with our captain’s score doubled in the 2022 season.\nFollowing the conclusion of the round, player values are recalibrated based on performance -— with increases for scores above their average and decreases for below-average performances. Our budget for the next round is our previous budget, plus the sum of our players’ value variations."
  },
  {
    "objectID": "blog/fantasy-football/index.html#data-wrangling",
    "href": "blog/fantasy-football/index.html#data-wrangling",
    "title": "Picking a fantasy football team",
    "section": "Data wrangling",
    "text": "Data wrangling\nLet’s talk about data structures: each round has a market, and each market is a list of players. A player is a structure like this:\n\n\nPlayer(\n│   round=0,\n│   player=42234,\n│   team=264,\n│   position=1,\n│   games=0,\n│   average=0.0,\n│   value=10.0,\n│   score=0.0,\n│   appreciation=0.0,\n│   minimum=4.53\n)\n\n\n\nLet’s get the list of markets for 2022 and flatten it into a single DataFrame:\n\n\nshape: (30_063, 10)\n┌───────┬────────┬──────┬──────────┬───┬───────┬───────┬──────────────┬─────────┐\n│ round ┆ player ┆ team ┆ position ┆ … ┆ value ┆ score ┆ appreciation ┆ minimum │\n╞═══════╪════════╪══════╪══════════╪═══╪═══════╪═══════╪══════════════╪═════════╡\n│ 1     ┆ 37424  ┆ 1371 ┆ 6        ┆ … ┆ 3.0   ┆ 0.0   ┆ 0.0          ┆ 0.0     │\n│ 1     ┆ 37646  ┆ 314  ┆ 3        ┆ … ┆ 5.0   ┆ 0.0   ┆ 0.0          ┆ 2.3     │\n│ 1     ┆ 37656  ┆ 266  ┆ 1        ┆ … ┆ 9.0   ┆ 0.0   ┆ 0.0          ┆ 4.08    │\n│ 1     ┆ 37788  ┆ 356  ┆ 1        ┆ … ┆ 4.0   ┆ 0.0   ┆ 0.0          ┆ 1.85    │\n│ …     ┆ …      ┆ …    ┆ …        ┆ … ┆ …     ┆ …     ┆ …            ┆ …       │\n│ 38    ┆ 121397 ┆ 286  ┆ 2        ┆ … ┆ 1.0   ┆ 0.0   ┆ 0.0          ┆ 0.0     │\n│ 38    ┆ 121398 ┆ 354  ┆ 4        ┆ … ┆ 1.0   ┆ 0.0   ┆ 0.0          ┆ 0.0     │\n│ 38    ┆ 121399 ┆ 354  ┆ 4        ┆ … ┆ 1.0   ┆ 0.0   ┆ 0.0          ┆ 0.0     │\n│ 38    ┆ 121400 ┆ 354  ┆ 5        ┆ … ┆ 1.0   ┆ 0.0   ┆ 0.0          ┆ 0.0     │\n└───────┴────────┴──────┴──────────┴───┴───────┴───────┴──────────────┴─────────┘\n\n\n\nNow, let’s focus on a specific player to illustrate our data while we wrangle it:\n\n\nshape: (38, 10)\n┌───────┬────────┬──────┬──────────┬───┬───────┬───────┬──────────────┬─────────┐\n│ round ┆ player ┆ team ┆ position ┆ … ┆ value ┆ score ┆ appreciation ┆ minimum │\n╞═══════╪════════╪══════╪══════════╪═══╪═══════╪═══════╪══════════════╪═════════╡\n│ 1     ┆ 42234  ┆ 264  ┆ 1        ┆ … ┆ 10.0  ┆ 0.0   ┆ 0.0          ┆ 4.53    │\n│ 2     ┆ 42234  ┆ 264  ┆ 1        ┆ … ┆ 7.93  ┆ 2.0   ┆ -2.07        ┆ 5.52    │\n│ 3     ┆ 42234  ┆ 264  ┆ 1        ┆ … ┆ 10.44 ┆ 11.0  ┆ 2.51         ┆ 4.75    │\n│ 4     ┆ 42234  ┆ 264  ┆ 1        ┆ … ┆ 10.44 ┆ 0.0   ┆ 0.0          ┆ 5.78    │\n│ …     ┆ …      ┆ …    ┆ …        ┆ … ┆ …     ┆ …     ┆ …            ┆ …       │\n│ 35    ┆ 42234  ┆ 264  ┆ 1        ┆ … ┆ 11.48 ┆ 0.0   ┆ -0.71        ┆ 3.55    │\n│ 36    ┆ 42234  ┆ 264  ┆ 1        ┆ … ┆ 11.51 ┆ 0.0   ┆ 0.03         ┆ 3.63    │\n│ 37    ┆ 42234  ┆ 264  ┆ 1        ┆ … ┆ 12.68 ┆ 0.0   ┆ 1.17         ┆ 9.29    │\n│ 38    ┆ 42234  ┆ 264  ┆ 1        ┆ … ┆ 11.06 ┆ 0.0   ┆ -1.62        ┆ 1.37    │\n└───────┴────────┴──────┴──────────┴───┴───────┴───────┴──────────────┴─────────┘\n\n\n\n\nFiltering participation\nPlayers will show up in the market for many rounds that they do not participate in. However, for our analysis, we are only interested in players that actually played a game in the round.\nEach player has a status field intended to indicate their participation in the round. However, this field is often inaccurate, likely due to the API data being updated before the round.\nOne solution is to keep only rows where there is an increase in the number of games the player has played:\n\n\nshape: (31, 3)\n┌───────┬────────┬───────┐\n│ round ┆ player ┆ games │\n╞═══════╪════════╪═══════╡\n│ 1     ┆ 42234  ┆ 0     │\n│ 2     ┆ 42234  ┆ 1     │\n│ 3     ┆ 42234  ┆ 2     │\n│ 5     ┆ 42234  ┆ 3     │\n│ …     ┆ …      ┆ …     │\n│ 35    ┆ 42234  ┆ 27    │\n│ 36    ┆ 42234  ┆ 28    │\n│ 37    ┆ 42234  ┆ 29    │\n│ 38    ┆ 42234  ┆ 30    │\n└───────┴────────┴───────┘\n\n\n\n\n\nImputing scores\nSimilarly, the player score field is often inaccurate, likely for the same reasons as the status field. Fortunately, the average field is reliable, allowing us to recover the score:\n\\[\n\\begin{align*}\n\\mathrm{Average}(\\mathbf{s}_{1:t})\n= \\frac{\\mathrm{Average}(\\mathbf{s}_{1:(t-1)}) + s_t}{2} \\\\\ns_t\n= 2\\mathrm{Average}(\\mathbf{s}_{1:t}) - \\mathrm{Average}(\\mathbf{s}_{1:(t-1)}),\n\\end{align*}\n\\]\nwhere \\(\\mathbf{s}\\) is the vector of scores for a given player across all rounds.\n\n\nshape: (31, 4)\n┌───────┬────────┬───────┬─────────┐\n│ round ┆ player ┆ score ┆ average │\n╞═══════╪════════╪═══════╪═════════╡\n│ 1     ┆ 42234  ┆ 2.0   ┆ 2.0     │\n│ 2     ┆ 42234  ┆ 11.0  ┆ 6.5     │\n│ 3     ┆ 42234  ┆ 9.5   ┆ 8.0     │\n│ 5     ┆ 42234  ┆ 8.6   ┆ 8.3     │\n│ …     ┆ …      ┆ …     ┆ …       │\n│ 35    ┆ 42234  ┆ 4.73  ┆ 4.82    │\n│ 36    ┆ 42234  ┆ 5.1   ┆ 4.96    │\n│ 37    ┆ 42234  ┆ 4.62  ┆ 4.79    │\n│ 38    ┆ 42234  ┆ 4.79  ┆ 4.79    │\n└───────┴────────┴───────┴─────────┘\n\n\n\n\n\nAdding fixtures\nLet’s fetch the list of fixtures to enrich our dataset. A fixture is an object like:\n\n\nFixture(round=1, home=282, away=285)\n\n\n\nLet’s consolidate these fixtures into a single DataFrame and then pivot them into a long format:\n\n\nshape: (760, 4)\n┌───────┬──────┬────────┬──────┐\n│ round ┆ team ┆ versus ┆ home │\n╞═══════╪══════╪════════╪══════╡\n│ 1     ┆ 282  ┆ 285    ┆ 1    │\n│ 1     ┆ 266  ┆ 277    ┆ 1    │\n│ 1     ┆ 276  ┆ 293    ┆ 1    │\n│ 1     ┆ 373  ┆ 262    ┆ 1    │\n│ …     ┆ …    ┆ …      ┆ …    │\n│ 38    ┆ 286  ┆ 354    ┆ 0    │\n│ 38    ┆ 276  ┆ 290    ┆ 0    │\n│ 38    ┆ 294  ┆ 1371   ┆ 0    │\n│ 38    ┆ 263  ┆ 293    ┆ 0    │\n└───────┴──────┴────────┴──────┘\n\n\n\nFinally, let’s join this data to our dataset:\n\n\nshape: (31, 5)\n┌───────┬────────┬──────┬────────┬──────┐\n│ round ┆ player ┆ team ┆ versus ┆ home │\n╞═══════╪════════╪══════╪════════╪══════╡\n│ 1     ┆ 42234  ┆ 264  ┆ 263    ┆ 0    │\n│ 2     ┆ 42234  ┆ 264  ┆ 314    ┆ 1    │\n│ 3     ┆ 42234  ┆ 264  ┆ 275    ┆ 0    │\n│ 5     ┆ 42234  ┆ 264  ┆ 280    ┆ 0    │\n│ …     ┆ …      ┆ …    ┆ …      ┆ …    │\n│ 35    ┆ 42234  ┆ 264  ┆ 262    ┆ 0    │\n│ 36    ┆ 42234  ┆ 264  ┆ 354    ┆ 1    │\n│ 37    ┆ 42234  ┆ 264  ┆ 294    ┆ 0    │\n│ 38    ┆ 42234  ┆ 264  ┆ 282    ┆ 1    │\n└───────┴────────┴──────┴────────┴──────┘\n\n\n\n\n\nAligning variables\nIn our subsequent analysis, the average field will exclude the score from the given round. Additionally, the appreciation field will be calculated in relation to the round’s score.\n\n\nshape: (31, 6)\n┌───────┬────────┬─────────┬───────┬───────┬──────────────┐\n│ round ┆ player ┆ average ┆ value ┆ score ┆ appreciation │\n╞═══════╪════════╪═════════╪═══════╪═══════╪══════════════╡\n│ 1     ┆ 42234  ┆ 0.0     ┆ 10.0  ┆ 2.0   ┆ -2.07        │\n│ 2     ┆ 42234  ┆ 2.0     ┆ 7.93  ┆ 11.0  ┆ 2.51         │\n│ 3     ┆ 42234  ┆ 6.5     ┆ 10.44 ┆ 9.5   ┆ 1.25         │\n│ 5     ┆ 42234  ┆ 8.0     ┆ 11.69 ┆ 8.6   ┆ 0.73         │\n│ …     ┆ …      ┆ …       ┆ …     ┆ …     ┆ …            │\n│ 35    ┆ 42234  ┆ 4.91    ┆ 11.48 ┆ 4.73  ┆ 0.03         │\n│ 36    ┆ 42234  ┆ 4.82    ┆ 11.51 ┆ 5.1   ┆ 1.17         │\n│ 37    ┆ 42234  ┆ 4.96    ┆ 12.68 ┆ 4.62  ┆ -1.62        │\n│ 38    ┆ 42234  ┆ 4.79    ┆ 11.06 ┆ 4.79  ┆ 0.0          │\n└───────┴────────┴─────────┴───────┴───────┴──────────────┘"
  },
  {
    "objectID": "blog/fantasy-football/index.html#team-picking",
    "href": "blog/fantasy-football/index.html#team-picking",
    "title": "Picking a fantasy football team",
    "section": "Team picking",
    "text": "Team picking\nNow let’s solve the problem of picking the best team a given market. Let $ $ be the set of valid formations, then for each formation \\(f \\in \\mathcal{F}\\), solve:\n\\[\n\\begin{equation*} \\begin{array}{ll@{}ll}\n\\text{maximize} & \\displaystyle \\hat{\\mathbf{s}}^T \\mathbf{x}, & \\mathbf{x} \\in \\{\\mathbf{0}, \\mathbf{1}\\} \\\\\n\\text{subject to}\n& \\displaystyle \\mathbf{v}^T \\mathbf{x} \\leq b \\\\\n& \\displaystyle \\mathbf{P}^T \\mathbf{x} = f, \\\\\n\\end{array} \\end{equation*}\n\\]\nwhere\n\\(\\mathbf{x}\\) is a variable vector of player picks in the market;\n\\(\\hat{\\mathbf{s}}\\) is the vector of predicted player scores in the market;\n\\(b\\) is our available budget for that round;\n\\(\\mathbf{P}\\) is the matrix of dummy-encoded player formations in the market.\nFinally, take the solution with the highest objective.\n\nclass Problem(BaseModel):\n    scores: List[float]\n    values: List[float]\n    budget: float\n    positions: List[List[int]]\n    formations: List[Formation]\n\n    def solve(self) -&gt; List[pulp.LpSolution]:\n        formations = [list(f.model_dump().values()) for f in self.formations]\n        problems = [self.construct(f) for f in formations]\n        [p.solve(pulp.COIN(msg=False)) for p in problems]\n        objectives = [p.objective.value() for p in problems]\n        best = np.argmax(np.array(objectives))\n        solution = problems[best]\n        variables = [v.value() for v in solution.variables()]\n        picks = np.array(variables)\n        return picks\n\n    def construct(self, formation: List[int]) -&gt; pulp.LpProblem:\n        n = len(self.scores)\n        m = len(formation)\n        problem = pulp.LpProblem(\"team_picking\", pulp.LpMaximize)\n        indexes = [\"pick_\" + str(i).zfill(len(str(n))) for i in range(n)]\n        picks = [pulp.LpVariable(i, cat=pulp.const.LpBinary) for i in indexes]\n        problem += pulp.lpDot(picks, self.scores)\n        problem += pulp.lpDot(picks, self.values) &lt;= self.budget\n        for i in range(m):\n            problem += pulp.lpDot(picks, self.positions[i]) == formation[i]\n        return problem\n\n\nBacktesting\nBy solving the team picking problem for all rounds, we can backtest our performance in the season. Before backtesting, let’s get the set of valid formations \\(\\mathcal{F}\\):\n\n\n[\n│   Formation(goalkeeper=1, defender=3, winger=0, midfielder=4, forward=3, coach=1),\n│   Formation(goalkeeper=1, defender=3, winger=0, midfielder=5, forward=2, coach=1),\n│   Formation(goalkeeper=1, defender=2, winger=2, midfielder=3, forward=3, coach=1),\n│   Formation(goalkeeper=1, defender=2, winger=2, midfielder=4, forward=2, coach=1),\n│   Formation(goalkeeper=1, defender=2, winger=2, midfielder=5, forward=1, coach=1),\n│   Formation(goalkeeper=1, defender=3, winger=2, midfielder=3, forward=2, coach=1),\n│   Formation(goalkeeper=1, defender=3, winger=2, midfielder=4, forward=1, coach=1)\n]\n\n\n\nKnowing our formation constraints, we’re ready to backtest. Starting with a budget of C$ 100, for each round let’s:\n\nPredict each player’s score based on their performance on previous rounds;\nPick the team with the best total score;\nAdd the sum of the team player’s appreciation to our budget.\n\n\ndef backtest(\n    players: pl.DataFrame, predict: Callable, initial_budget: float = 100.0\n) -&gt; pl.DataFrame:\n    rounds = players.get_column(\"round\").max()\n    budget = [None] * rounds\n    teams = [None] * rounds\n    budget[0] = initial_budget\n    for round in range(rounds):\n        if round &gt; 0:\n            budget[round] = budget[round - 1] + appreciation\n        data = players.filter(pl.col(\"round\") &lt; round + 1)\n        candidates = players.filter(pl.col(\"round\") == round + 1)\n        candidates = predict(data, candidates)\n        problem = Problem(\n            scores=candidates.get_column(\"prediction\"),\n            values=candidates.get_column(\"value\"),\n            positions=candidates.get_column(\"position\").to_dummies(),\n            budget=budget[round],\n            formations=formations,\n        )\n        picks = problem.solve()\n        team = candidates.filter(picks == 1)\n        teams[round] = team\n        appreciation = team.get_column(\"appreciation\").sum()\n    teams = pl.concat(teams)\n    return teams\n\nBefore exploring predictions, we’ll begin with a few hypothetical backtests using actual observed scores for team selection. Backtesting this strategy, this is our team in the first round:\n\n\nshape: (12, 13)\n┌───────┬────────┬──────┬──────────┬───┬─────────┬────────┬──────┬────────────┐\n│ round ┆ player ┆ team ┆ position ┆ … ┆ minimum ┆ versus ┆ home ┆ prediction │\n╞═══════╪════════╪══════╪══════════╪═══╪═════════╪════════╪══════╪════════════╡\n│ 1     ┆ 71571  ┆ 356  ┆ 1        ┆ … ┆ 3.19    ┆ 1371   ┆ 1    ┆ 11.0       │\n│ 1     ┆ 42145  ┆ 294  ┆ 2        ┆ … ┆ 2.75    ┆ 290    ┆ 1    ┆ 15.8       │\n│ 1     ┆ 105584 ┆ 264  ┆ 2        ┆ … ┆ 2.75    ┆ 263    ┆ 0    ┆ 10.5       │\n│ 1     ┆ 107110 ┆ 280  ┆ 3        ┆ … ┆ 2.3     ┆ 286    ┆ 0    ┆ 14.9       │\n│ …     ┆ …      ┆ …    ┆ …        ┆ … ┆ …       ┆ …      ┆ …    ┆ …          │\n│ 1     ┆ 39148  ┆ 282  ┆ 5        ┆ … ┆ 7.2     ┆ 285    ┆ 1    ┆ 18.9       │\n│ 1     ┆ 89840  ┆ 276  ┆ 5        ┆ … ┆ 5.42    ┆ 293    ┆ 1    ┆ 27.1       │\n│ 1     ┆ 104530 ┆ 294  ┆ 5        ┆ … ┆ 2.3     ┆ 290    ┆ 1    ┆ 11.0       │\n│ 1     ┆ 97341  ┆ 276  ┆ 6        ┆ … ┆ 0.0     ┆ 293    ┆ 1    ┆ 9.52       │\n└───────┴────────┴──────┴──────────┴───┴─────────┴────────┴──────┴────────────┘\n\n\n\nAnd we can plot out cumulative performance during the season:\n\n\n\n\n\n\n\n\n\nThis might seem like a perfect campaign at first, but it’s possible that, early in the season, we didn’t have enough budget to pick the best scoring teams. To test this hypothesis, we backtest the same strategy with unlimited budget from the start:\n\n\n\n\n\n\n\n\n\nBoth runs are nearly identical, which is evidence that focusing on appreciation is not so important if we have accurate predictions for the scores. If we predict scores perfectly, we get a near perfect run.\nTo put our backtests into perspective, the 2022 season champion had a total score of 3434.37. This is very impressive and not very far from the near perfect run."
  },
  {
    "objectID": "blog/fantasy-football/index.html#score-prediction",
    "href": "blog/fantasy-football/index.html#score-prediction",
    "title": "Picking a fantasy football team",
    "section": "Score prediction",
    "text": "Score prediction\nFor each round, we must predict \\(\\hat{s}\\), the vector of score predictions, using data from previous rounds.\nHowever, during the first round, we don’t have any previous data to train our model. In this case, we need to include prior information. One way to do that would be to use data from previous seasons. However, we know a variable where this information is already encoded: the player value. Each season starts with players valued according to their past performance. Knowing this, all our models start with \\(\\hat{s} = v\\) in the first round.\nLet’s use Bambi (Capretto et al. 2022) and its default priors to fit our models. We won’t delve into convergence diagnostics, since we are more interested in the average of the predictive posteriors and the backtest itself is measure of the prediction quality.\nOne question that arises here is: why not use non-parametric models such as gradient boosted trees or neural nets? After some experimentation, I concluded they are not a good fit for this problem: either because they assume independence between observations, or because they are too data hungry. Also, tuning these models for backtests might lead us into a rabbit hole (Bailey et al. 2013).\n\nPlayer average\n\\[\n\\begin{align*}\n\\mathbf{\\hat{s}} = \\mathbf{Z} \\mathbf{\\beta} \\\\\n\\mathbf{s} \\sim N(\\mathbf{\\hat{s}}, \\sigma),\n\\end{align*}\n\\]\nwhere\n\\(\\mathbf{Z}\\) is a dummy-encoded matrix of players;\n\\(\\mathbf{\\beta}\\) is a vector of parameters for each player.\nIn this model, \\(\\mathbf{\\beta}\\) is simply a vector of player averages. Let’s also consider that players that show up in the middle of the season have an average of zero before their first round. This will be our baseline model.\n\n\n\n\n\n\n\n\n\n\n\nPlayer random effects\n\\[\n\\begin{align*}\n\\mathbf{\\hat{s}} = \\alpha + \\mathbf{Z} \\mathbf{b} \\\\\n\\mathbf{b} \\sim N(0, \\sigma_b),\n\\end{align*}\n\\]\nwhere\n\\(\\alpha\\) is an intercept and\n\\(\\mathbf{b}\\) is a vector of player random effects.\nThis model performs significantly better than the average model, possibly because of the partial pooling between the random effects, that pulls large effects towards the overall mean (Clark 2019). In our dataset, it’s common for players that played one or two games to have large averages by chance.\n\n\n\n\n\n\n\n\n\n\n\nFixture mixed effects\n\\[\n\\mathbf{\\hat{s}} = \\alpha + \\mathbf{X} \\mathbf{\\beta} + \\mathbf{Z} \\mathbf{b},\n\\]\nwhere\n\\(\\mathbf{X}\\) is a matrix of the dummy-encoded fixture variables: the player team, whether they are playing at home, and their adversary team variables; \\(\\mathbf{\\beta}\\) is a vector of fixed effects.\nThis model brings more context to our predictions. It also provides a reasonable way to predict a new player, by setting their \\(b = 0\\) (the mean of the random effects). However, it does not improve significantly over our random effects model."
  },
  {
    "objectID": "blog/fantasy-football/index.html#conclusion",
    "href": "blog/fantasy-football/index.html#conclusion",
    "title": "Picking a fantasy football team",
    "section": "Conclusion",
    "text": "Conclusion\nWe developed a comprehensive framework for the fantasy football team picking problem. There are more ideas we could explore to improve our chances of winning:\n\nenriching our data and models with player scouts;\nincluding more information in our priors;\ntesting strategies that balance predicted score and appreciation;\nfurther model diagnostics.\n\nHowever, I suppose expert human player predictions have a certain edge over those of hobbyist statistical models in fantasy leagues, due to the fact that there are all sorts of relevant data unavailable in public datasets.\nAt least, this seems to be the case for brazilian soccer, also known as “a little box of surprises”."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "This is a list of texts I enjoyed reading and/or cited in my blog:\n\n\nBailey, David H., Jonathan M. Borwein, Marcos Lopez de Prado, and Qiji Jim Zhu. 2013. “The Probability of Back-Test over-Fitting.” SSRN Electronic Journal. https://doi.org/10.2139/ssrn.2326253.\n\n\nBartoš, František, Alexandra Sarafoglou, Henrik R Godmann, Amir Sahrani, David Klein Leunk, Pierre Y Gui, David Voss, et al. 2023. “Fair Coins Tend to Land on the Same Side They Started: Evidence from 350,757 Flips.”\n\n\nCapretto, Tomás, Camen Piho, Ravin Kumar, Jacob Westfall, Tal Yarkoni, and Osvaldo A Martin. 2022. “Bambi: A Simple Interface for Fitting Bayesian Linear Models in Python.” Journal of Statistical Software 103 (15): 1–29. https://doi.org/10.18637/jss.v103.i15.\n\n\nClark, Michael. 2019. “Michael Clark: Shrinkage in Mixed Effects Models.” https://m-clark.github.io/posts/2019-05-14-shrinkage-in-mixed-models/.\n\n\nHartshorne, Joshua K. 2014. “Data.” OSF. osf.io/4xp3g.\n\n\nHartshorne, Joshua K., and Laura T. Germine. 2015. “When Does Cognitive Functioning Peak? The Asynchronous Rise and Fall of Different Cognitive Abilities Across the Life Span.” Psychological Science 26 (4): 433–43. https://doi.org/10.1177/0956797614567339.\n\n\nKleinberg, Jon. 2003. Data Min. Knowl. Discov. 7 (4): 373–97."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Luís Assunção",
    "section": "",
    "text": "Decomposable non-monotonic models\n\n\n\n\n\n\n\n\nNov 17, 2023\n\n\n\n\n\n\n\nPicking a fantasy football team\n\n\n\n\n\n\n\n\nSep 21, 2023\n\n\n\n\n\n\nNo matching items"
  }
]